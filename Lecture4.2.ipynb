{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial helps you get familiar with general steps of Natural Language Processing. We will use NLTK package in Python. To install NLTK package, use command **\"pip install nltk\"**. \n",
    "\n",
    "If you are running NLTK for the first time, you might have trouble importing some modules. Try download NLTK modules using **nltk.download(\"module_name\")**. For example, you can download stopwords by **nltk.download(\"stopwords\")**. \n",
    "\n",
    "Optionally, you can also download all data from NLTK, this step takes time\n",
    "As you as you download NLTK once, the data will reside in your machine: **nltk.download(\"all\")**  # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five documents related to nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents\n",
    "doc1 = 'By resolving the reference resolution in the design document, the shallow natural language processing technique increases the amount of design information content extracted'\n",
    "doc2 = 'It is also stimulating cross-fertilisation of ideas between researchers in natural language processing, information retrieval and artificial intelligence'\n",
    "doc3 = 'We discuss how these findings can be integrated into a natural language processing system'\n",
    "doc4 = 'In this paper, we discuss how a natural language processing system can take advantage of this information to understand pronominal references to quantified expressions'\n",
    "doc5 = 'Natural language processing has also provided some tools for computerised scoring of essays, particularly relevant in large-scale language testing programs'\n",
    "\n",
    "# corpus, each element is a string (document)\n",
    "documents = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing steps consist of tokenization, lowercase, removing stopwords, stemming and Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# lower and tokenize\n",
    "tokenized = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# document 1\n",
    "# tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get stop words of English\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# you can add anything\n",
    "punctuations = list('''!()-[]{};:'\"\\,<>./?@#$%^&*_~''')\n",
    "\n",
    "stop = stop + punctuations\n",
    "\n",
    "# remove stop words and punctuation\n",
    "docs = [[word for word in words if word not in stop] \n",
    "        for words in tokenized]\n",
    "\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# initialize stemmer and lemmatizer\n",
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# stemming \n",
    "docs_stem = [[porter.stem(word) for word in words]\n",
    "               for words in docs]\n",
    "\n",
    "# lemmatization\n",
    "docs_lemma = [[wordnet.lemmatize(word) for word in doc]\n",
    "                for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mice\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "# print(porter.stem('mice'))\n",
    "# print(wordnet.lemmatize('mice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a Vocabulary for our Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets flatten the docs_lemma\n",
    "fatten_docs = [word for doc in docs_lemma for word in doc]\n",
    "\n",
    "# use set to remove duplicates\n",
    "vocabulary = sorted(list(set(fatten_docs)))\n",
    "\n",
    "# print (vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your own Bag of Words function. Then compare with the function in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of word for document 1: \n",
      "\n",
      "['advantage', 'also', 'amount', 'artificial', 'computerised', 'content', 'cross-fertilisation', 'design', 'discus', 'document', 'essay', 'expression', 'extracted', 'finding', 'idea', 'increase', 'information', 'integrated', 'intelligence', 'language', 'large-scale', 'natural', 'paper', 'particularly', 'processing', 'program', 'pronominal', 'provided', 'quantified', 'reference', 'relevant', 'researcher', 'resolution', 'resolving', 'retrieval', 'scoring', 'shallow', 'stimulating', 'system', 'take', 'technique', 'testing', 'tool', 'understand']\n",
      "\n",
      "[0. 0. 1. 0. 0. 1. 0. 2. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def bow_vectorize(doc, vocabulary):\n",
    "    \"\"\"\n",
    "    doc: one document\n",
    "    vocabulary: vocabulary\n",
    "    return: BOW vector\n",
    "    \"\"\"\n",
    "    # initialize a list\n",
    "    doc_vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    # word count \n",
    "    word_count = dict(Counter(doc))\n",
    "    \n",
    "    # update vector\n",
    "    for i, w in enumerate(vocabulary):\n",
    "        if w in word_count:\n",
    "            doc_vector[i] = word_count[w]\n",
    "    \n",
    "    return doc_vector\n",
    "\n",
    "bow_matrix = [bow_vectorize(doc, vocabulary) for doc in docs_lemma]\n",
    "\n",
    "print (\"Bag of word for document 1: \\n\")\n",
    "print (vocabulary)\n",
    "print ()\n",
    "print (bow_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, feature extraction.text can implement BOW and TF-IDF.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of word for document 1: \n",
      "\n",
      "['advantage', 'also', 'amount', 'artificial', 'computerised', 'content', 'cross-fertilisation', 'design', 'discus', 'document', 'essay', 'expression', 'extracted', 'finding', 'idea', 'increase', 'information', 'integrated', 'intelligence', 'language', 'large-scale', 'natural', 'paper', 'particularly', 'processing', 'program', 'pronominal', 'provided', 'quantified', 'reference', 'relevant', 'researcher', 'resolution', 'resolving', 'retrieval', 'scoring', 'shallow', 'stimulating', 'system', 'take', 'technique', 'testing', 'tool', 'understand']\n",
      "\n",
      "[0 0 1 0 0 1 0 2 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# define tokenizer function, which will be used in CountVectorizer()\n",
    "def lemmatize(doc):\n",
    "    return [wordnet.lemmatize(word) for word in word_tokenize(doc.lower())]\n",
    "\n",
    "# initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=stopwords.words('english'),\n",
    "                                   vocabulary=vocabulary,\n",
    "                                   tokenizer=lemmatize)\n",
    "\n",
    "# transform documents\n",
    "feature_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print (\"Bag of word for document 1: \\n\")\n",
    "print (vocabulary)\n",
    "print ()\n",
    "print (feature_matrix.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize a tfidf\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                                   vocabulary=vocabulary)\n",
    "# fit documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# sparse matrix to dense\n",
    "tfidf_matrix = tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF for document 1: \n",
      "\n",
      "['advantage', 'also', 'amount', 'artificial', 'computerised', 'content', 'cross-fertilisation', 'design', 'discus', 'document', 'essay', 'expression', 'extracted', 'finding', 'idea', 'increase', 'information', 'integrated', 'intelligence', 'language', 'large-scale', 'natural', 'paper', 'particularly', 'processing', 'program', 'pronominal', 'provided', 'quantified', 'reference', 'relevant', 'researcher', 'resolution', 'resolving', 'retrieval', 'scoring', 'shallow', 'stimulating', 'system', 'take', 'technique', 'testing', 'tool', 'understand']\n",
      "\n",
      "[[0.         0.         0.26603192 0.         0.         0.26603192\n",
      "  0.         0.53206384 0.         0.26603192 0.         0.\n",
      "  0.26603192 0.         0.         0.         0.17816468 0.\n",
      "  0.         0.12676564 0.         0.12676564 0.         0.\n",
      "  0.12676564 0.         0.         0.         0.         0.26603192\n",
      "  0.         0.         0.26603192 0.26603192 0.         0.\n",
      "  0.26603192 0.         0.         0.         0.26603192 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print (\"TFIDF for document 1: \\n\")\n",
    "print (vocabulary)\n",
    "print ()\n",
    "print (tfidf_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         4.69041576 4.35889894 4.69041576 5.19615242]\n",
      " [4.69041576 0.         3.60555128 4.24264069 4.35889894]\n",
      " [4.35889894 3.60555128 0.         3.31662479 4.        ]\n",
      " [4.69041576 4.24264069 3.31662479 0.         4.79583152]\n",
      " [5.19615242 4.35889894 4.         4.79583152 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# BOW\n",
    "print(euclidean_distances(bow_matrix, bow_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.32287677 1.32765676 1.33585782 1.3525131 ]\n",
      " [1.32287677 0.         1.2763048  1.28961429 1.24170497]\n",
      " [1.32765676 1.2763048  0.         1.1724174  1.25690387]\n",
      " [1.33585782 1.28961429 1.1724174  0.         1.33039625]\n",
      " [1.3525131  1.24170497 1.25690387 1.33039625 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TFIDF\n",
    "print(euclidean_distances(tfidf_matrix, tfidf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.27216553 0.26726124 0.31497039 0.22866478]\n",
      " [0.27216553 1.         0.32732684 0.3086067  0.35007002]\n",
      " [0.26726124 0.32732684 1.         0.50507627 0.3666794 ]\n",
      " [0.31497039 0.3086067  0.50507627 1.         0.25928149]\n",
      " [0.22866478 0.35007002 0.3666794  0.25928149 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# BOW\n",
    "print(cosine_similarity(bow_matrix, bow_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.12499852 0.11866376 0.10774194 0.08535416]\n",
      " [0.12499852 1.         0.18552303 0.16844749 0.22908439]\n",
      " [0.11866376 0.18552303 1.         0.31271873 0.21009633]\n",
      " [0.10774194 0.16844749 0.31271873 1.         0.11502291]\n",
      " [0.08535416 0.22908439 0.21009633 0.11502291 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TFIDF\n",
    "print(cosine_similarity(tfidf_matrix, tfidf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.719px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
